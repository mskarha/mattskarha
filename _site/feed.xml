<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-02T18:16:28-04:00</updated><id>http://localhost:4000/</id><title type="html">Matt Skarha</title><subtitle>Your description here</subtitle><entry><title type="html">eggDJ: A Portable Real-Time Music Augmentation System</title><link href="http://localhost:4000/20191210/lorem-ipsum" rel="alternate" type="text/html" title="eggDJ: A Portable Real-Time Music Augmentation System" /><published>2019-12-10T00:00:00-05:00</published><updated>2019-12-10T00:00:00-05:00</updated><id>http://localhost:4000/20191210/lorem-ipsum</id><content type="html" xml:base="http://localhost:4000/20191210/lorem-ipsum">&lt;h1 id=&quot;eggdj-a-portable-real-time-music-augmentation-system&quot;&gt;eggDJ: A Portable Real-Time Music Augmentation System&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;200&quot; src=&quot;/assets/img/eggDJ.jpg&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;From time to time, I catch myself energetically tapping my fingers to the beat of my music while on the bus and the metro. For a brief moment, I become so captivated by the groove in my headphones that I completely lose awareness of my surroundings. I find myself attempting to match the kick/snare pattern with my index and middle fingers as well as embellish the drums with rhythms of my own.&lt;/p&gt;

&lt;p&gt;This project is an attempt to bring those moments to life.&lt;/p&gt;

&lt;h2 id=&quot;live-augmentation-as-musical-practice&quot;&gt;Live Augmentation as Musical Practice&lt;/h2&gt;

&lt;p&gt;I would like to introduce the notion of live augmentation as a musical practice. Live augmentation, in this context (not to be confused with the music theory term of the same name describing the lengthening of a note), simply refers to adding your own flair to an existing piece of recorded music in real-time. This can be anything from layering vocal samples on top of a lofi SoundCloud house mix to practicing your jazz guitar comping by playing along with an famous jazz recording. The benefits of such practice when compared with the traditional practice of acoustic instruments are threefold: it is often a much more accessible approach to music, it can have interesting pedagogical effects, and it can breathe new life into an existing piece of music, opening up a new realm of creativity.&lt;/p&gt;

&lt;p&gt;Take the example of the guitarist comping along with a Miles Davis record. It is not necessarily an easy task to get, for example, a trumpet player, a bassist, and a drummer in a particular room at a particular time a) that you get along with, b) that are at a similar skill level as you, c) have similar musical goals as you, and d) without being too loud as to disturb others. Not to mention all the equipment hauling and level mixing that needs to be done. By simply playing along with a favorite record, the guitarist can often accomplish many of their musical goals much more easily.&lt;/p&gt;

&lt;p&gt;Additionally, many researchers in the field of Accessible Digital Musical Instruments, or ADMIs, (that seek to build musical control interfaces for persons with physical or mental disabilities) take the “augmentation” approach due to its low barrier to entry and retention of musical nuance [1].&lt;/p&gt;

&lt;p&gt;Live augmentation can also be a very pedagogical tool. If we play along with a recording that is deemed to “have more merit” within a particular tradition of music, we can often learn more easily by engaging directly with the music as compared to passive listening. The obvious example is the jazz guitarist emulating the comping technique of say, Freddie Green, while playing along with a Count Basie record, in an attempt to improve their comping skills. By inserting their own musical creativity into the piece, the guitarist is also, by definition, creating a new piece of music that has infinite potential for innovation.&lt;/p&gt;

&lt;h2 id=&quot;design-goals&quot;&gt;Design Goals&lt;/h2&gt;

&lt;p&gt;For this project, I wanted to implement a real-time music augmentation system that I can use on my way to class. This portability constraint made the project much more interesting than traditional music technology hardware, that often sits in a studio and requires a desktop computer for interaction.&lt;/p&gt;

&lt;p&gt;I had the following goals in mind while I designed eggDJ:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;it had to be able to play music from any streaming service (e.g. Spotify, SoundCloud, YouTube)&lt;/li&gt;
  &lt;li&gt;it had to have the ability to layer any digital sound with a basic sampler mechanism&lt;/li&gt;
  &lt;li&gt;the entire system should fit in a jacket pocket wirelessly&lt;/li&gt;
  &lt;li&gt;it should have zero visual aspect (only hearing and touch)&lt;/li&gt;
  &lt;li&gt;no one should be able to tell what you are doing (don’t wanna look weird on the bus)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hardware&quot;&gt;Hardware&lt;/h2&gt;

&lt;p&gt;An initial reaction to these design goals might be to simply build an Android/iPhone app where one can trigger samples by tapping the screen. While this would be fairly straightforward, it doesn’t accomplish all of the goals well because it would be difficult to interact with while not looking at it. 
&lt;img height=&quot;100&quot; align=&quot;right&quot; src=&quot;/assets/img/pi.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I decided to use a Raspberry Pi 3 with an &lt;a href=&quot;http://www.audioinjector.net/rpi-zero&quot;&gt;Audio Injector Zero Shield&lt;/a&gt; to implement the hardware side. It is notoriously difficult to get audio into the Pi (a simple Google search will confirm this, PureData’s website has the &lt;a href=&quot;https://puredata.info/docs/raspberry-pi&quot;&gt;best list of working soundcards&lt;/a&gt; that I was able to find). Even with the Audio Injector Zero, it still took several hours to figure out how to get audio in and out using the sound card. 
&lt;img height=&quot;100&quot; align=&quot;right&quot; src=&quot;/assets/img/zero.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to trigger the samples, I just decided to use 5 simple pushbuttons, one for each finger. The simple control interface makes the eggDJ very intuitive with a low entry-fee.&lt;/p&gt;

&lt;p&gt;Unfortunately, when I tried to use the Audio Injector Shield (which takes over all 40 of the Pi’s GPIO pins) in tandem with buttons attached to the GPIO pins, the Pi crashed. Thus, in order to get the pushbutton data into the Pi, I used a Teensy 3.2 interfaced via a micro-USB cable. The Teensy 3.2 has pullup resistors built-in to all 21 digital I/O pins, making it preferable compared to an Arduino Uno.&lt;/p&gt;

&lt;h2 id=&quot;software&quot;&gt;Software&lt;/h2&gt;

&lt;p&gt;On the software side of things, the most straightforward way to trigger samples over an audio input on a Raspberry Pi is using Pure Data (MSP’s Vanilla version). My patch (shown below) implements a simple sample selection mechanism, where the user cycles through an arbitrary number (in this case, ten) of samples on a given channel (finger) by pressing the thumb button.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;300&quot; src=&quot;/assets/img/tekmit.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;My Arduino code (below) simply writes which button was pushed to a serial stream which is read by Pd’s comport object. More details are in the code comments and ReadMe file.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://create.arduino.cc/editor/mskarha/742d57c9-5db0-4dcd-8c61-98dfa7ed13e3/preview?embed&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=idyqRyKqyLU&quot;&gt;https://www.youtube.com/watch?v=idyqRyKqyLU&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Frid, Emma. “Accessible Digital Musical Instruments - a Survey of Inclusive Instruments Presented at the NIME, SMC and ICMC Conferences.” ICMC (2018).&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">eggDJ: A Portable Real-Time Music Augmentation System</summary></entry><entry><title type="html">Cooperative Listening Devices with Distributed and Wearable Microphone Arrays</title><link href="http://localhost:4000/20190809/micarrays" rel="alternate" type="text/html" title="Cooperative Listening Devices with Distributed and Wearable Microphone Arrays" /><published>2019-08-09T00:00:00-04:00</published><updated>2019-08-09T00:00:00-04:00</updated><id>http://localhost:4000/20190809/micarrays</id><content type="html" xml:base="http://localhost:4000/20190809/micarrays">&lt;h1 id=&quot;cooperative-listening-devices-with-distributed-and-wearable-microphone-arrays&quot;&gt;Cooperative Listening Devices with Distributed and Wearable Microphone Arrays&lt;/h1&gt;

&lt;p&gt;I was fortunate enough to spend the past two summers (‘18 &amp;amp; ‘19) as a research assistant in the lab of &lt;a href=&quot;http://ryanmcorey.com/&quot;&gt;Ryan Corey, PhD&lt;/a&gt;. When I was working with him, he was a doctoral candidate in Electrical and Computer Engineering at the University of Illinois at Urbana-Champaign. This post summarizes our work.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;300&quot; src=&quot;/assets/img/room1.jpg&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Hearing aids don’t work well in noisy, reverberant environments where lots of people are talking at once (e.g. a crowded bar, conference poster session, etc.). Ryan, a hearing aid user himself, devoted his entire Ph.D. to improving the performance of hearing aids in noisy environments. In doing this, he also helped to develop what he calls ‘augmented listening (AL)’ technologies. You can read more about augmented listening technologies &lt;a href=&quot;https://publish.illinois.edu/augmentedlistening/what-is-augmented-listening/&quot;&gt;here&lt;/a&gt;, but in general, they can be grouped into four categories of products:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;traditional hearing aids&lt;/li&gt;
  &lt;li&gt;personal sound amplification products (&lt;a href=&quot;https://en.wikipedia.org/wiki/Personal_sound_amplification_product&quot;&gt;PSAPs&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;smart headphones, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Hearables&quot;&gt;“hearables”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;virtual, augmented, and mixed reality headsets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Much of Ryan’s work, where the primarily application is traditional hearing aids, can be readily extended to these other AL technologies.&lt;/p&gt;

&lt;h2 id=&quot;real-world-sound-de-mixing-and-re-mixing&quot;&gt;Real-World Sound De-mixing and Re-mixing&lt;/h2&gt;

&lt;p&gt;In an ideal world, a hearing aid user could turn up the volume of a particular sound source (a person they are conversing with) and turn down the volume of background noise (mechanical noise, other people talking, loud music, etc.). A perfect AL system would work like a mixing board in a recording studio: each sound source would have its own slider, and the user — or an algorithm — would decide how much of each source should end up in the final mix.&lt;/p&gt;

&lt;p&gt;&lt;img height=&quot;100&quot; align=&quot;right&quot; src=&quot;/assets/img/fader.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Developing such a system is not a trivial task, as you might imagine. The system would have to continuously do the following, in real-time:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identify all sources of sound in a given environment&lt;/li&gt;
  &lt;li&gt;Identify the types of sounds that are coming from each source&lt;/li&gt;
  &lt;li&gt;Separate the sounds into their own channels&lt;/li&gt;
  &lt;li&gt;Apply appropriate equalization and dynamic range compression settings to make the sounds more ‘listenable’&lt;/li&gt;
  &lt;li&gt;Allow the user to adjust the gain of each source while preserving each sources’ spatial cues&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Source separation itself is a super difficult problem, but doing it in real-time for human listening (as opposed to machine listening) applications requires only a few milliseconds of delay between the microphone and the ear, making the problem orders of magnitude more difficult. Luckily, AL technologies are not constrained by having only two ears, the way humans are. We can use dozens or even hundreds of microphones (microphone arrays) to learn about a given auditory scene and process it accordingly.&lt;/p&gt;

&lt;h2 id=&quot;microphone-arrays&quot;&gt;Microphone Arrays&lt;/h2&gt;

&lt;p&gt;Though we may not think about it, we are constantly surrounded by microphones (see ‘A Word on Privacy’ section if this freaks you out as much as it did for me). Multiple microphones are built into our phones, laptops, smart speakers, hearing aids, smart headphones, audio conferencing equipment, and so much more. For example, the Amazon Echo has seven microphones and the Microsoft Kinect has four. These microphone arrays use subtle time-of-flight differences between signals at different sensors to determine what sounds are coming from what directions, track them as they move, and separate the signals spatially.&lt;/p&gt;

&lt;p&gt;In our proposed system, we used similar techniques to perform source separation by exploiting the redundancies of all the microphones in a given room. The source separation system, which would run on a powerful computer or the cloud, decides what sources are in the room, where they are, and how sound propagates from the sources to each user’s listening device. Then, each listening device uses this information to design its own beamformer that uses only its own nearby microphones. Beamformers are like flashlights but instead of emitting light in the form of a beam, we are collecting acoustic information for a given three-dimensional space.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;&lt;img height=&quot;200&quot; align=&quot;right&quot; src=&quot;/assets/img/room.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To test the proposed system, Ryan and I filled a medium-sized conference room with 160 lavelier microphones. There were four mannequins with 16 microphones mounted at various locations on their clothes (Ryan did some &lt;a href=&quot;https://publish.illinois.edu/augmentedlistening/acoustic-impulse-responses-for-wearable-audio-devices/&quot;&gt;other work&lt;/a&gt; on wearable microphone arrays) as well as 12 enclosures that simulated smart speakers distributed around the room that each had eight microphones. Ten loudspeakers were mounted roughly at head height to simulate a cocktail party scenario.&lt;/p&gt;

&lt;p&gt;We recorded both impulse responses (to learn about the room acoustics) and speech samples played at each of the ten speakers, as well as a mixture of the speakers to simulate a cocktail party scenario. The work resulted in a &lt;a href=&quot;https://arxiv.org/pdf/1912.05038.pdf&quot;&gt;CAMSAP paper&lt;/a&gt;, poster (below) and publicly-available &lt;a href=&quot;https://databank.illinois.edu/datasets/IDB-6216881#&quot;&gt;dataset&lt;/a&gt;. Check out some of the results on &lt;a href=&quot;http://ryanmcorey.com/demos/cooperative/cooperative_demo.html&quot;&gt;our demos page.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-word-on-privacy&quot;&gt;A Word on Privacy&lt;/h2&gt;

&lt;p&gt;Obviously, there are severe privacy concerns that have not been addressed in this work. For starters, a user would be able to listen to any sound source, even if it is on the other side of the room. Furthermore, a hacker would be able to hijack the contents of any conversation in a room for whatever purpose. Luckily, a great deal of research has been done into privacy-preserving inference algorithms (see Peter Kairouz’s &lt;a href=&quot;https://www.ideals.illinois.edu/bitstream/handle/2142/92686/KAIROUZ-DISSERTATION-2016.pdf?sequence=1&amp;amp;isAllowed=y&quot;&gt;Ph.D. thesis&lt;/a&gt;) where the algorithm could perform acoustic channel estimation without having direct access to the signal’s content. That is, there are ways of encrypting signals in a context such as an audio remixing system that would allow them to protect user privacy. Nonetheless, more research needs to be done into the appropriateness for such a system in a given context and its impact on user privacy before being implemented in a real-world setting.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;900&quot; src=&quot;/assets/img/camsap.png&quot; /&gt;
&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Cooperative Listening Devices with Distributed and Wearable Microphone Arrays</summary></entry><entry><title type="html">Integer Programming for Optimal Right Hand Guitar Fingerings</title><link href="http://localhost:4000/20181217/fingerings" rel="alternate" type="text/html" title="Integer Programming for Optimal Right Hand Guitar Fingerings" /><published>2018-12-17T00:00:00-05:00</published><updated>2018-12-17T00:00:00-05:00</updated><id>http://localhost:4000/20181217/fingerings</id><content type="html" xml:base="http://localhost:4000/20181217/fingerings">&lt;h2 id=&quot;integer-programming-for-optimal-right-hand-guitar-fingerings&quot;&gt;Integer Programming for Optimal Right Hand Guitar Fingerings&lt;/h2&gt;

&lt;p&gt;The Western canon of European classical music includes a substantial amount of scale-based, fingerstyle guitar music. When presented with a piece of sheet music, a classical guitarist must make a number of decisions regarding its performance. These decisions include notating the tablature, the left hand fingerings, and the right hand fingerings. This process, especially for right hand guitar fingerings, is often a trivial, yet cumbersome task.&lt;/p&gt;

&lt;p&gt;For example, consider the &lt;a href=&quot;https://youtu.be/dmc6KV0_UVM?t=273&quot;&gt;&lt;em&gt;III. Allegro Solemne&lt;/em&gt; movement&lt;/a&gt; from the 1921 piece “La Catedral” by Paraguayan composer Agustín Barrios. Due to its fast tempo, the performer must pay careful attention to the fingerings to adhere to proper technique while maintaining such a tempo. The problem then becomes how can we use math and computer science to determine optimal fingerings for this type of music?&lt;/p&gt;

&lt;p&gt;Bernd Tahon began to answer this question with this 2017 Master’s thesis &lt;a href=&quot;https://vibeserver.net/scripties/2017/Fingers%20to%20Frets%20-%20Master%20Thesis%20Bernd%20Tahon.pdf&quot;&gt;&lt;em&gt;Fingers to frets - A Mathematical Approach&lt;/em&gt;&lt;/a&gt; where he examined how to utilize math and computer science to optimize left hand finger assignments. In this project, we tackled the separate, yet related problem of right hand finger assignments.&lt;/p&gt;

&lt;p&gt;In fingerstyle music, the guitar is played using the thumb, index, middle, and ring fingers (not the pinky). At present, there is a notion within the classical guitar community of “proper right hand technique” which can be thought of as a set of rules that describe the physically optimal way of using the right hand to play scale-based, fingerstyle guitar music. These rules can be summarized as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Do not repeat fingers on consecutive notes&lt;/li&gt;
  &lt;li&gt;The thumb plays the bassline&lt;/li&gt;
  &lt;li&gt;Fingers stay in natural resting position&lt;/li&gt;
  &lt;li&gt;Avoid backwards crossings&lt;/li&gt;
  &lt;li&gt;Avoid ring-middle-ring alternation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We decided to formulate this problem as an integer program where each of these rules are a constraint imposed by the model. The integer program, then, consists of a minimization of a sum of penalty terms that are incurred by violating the “soft” constraints. The complete integer program can be found below:&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%5Ctext%7Bminimize%7D&quot; alt=&quot;\text{minimize}&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p%5E%7BBC%7D_i%20%2B%20p%5ET_i%20%2B%20%5Cfrac%7B1%7D%7B2%7D%20p%5EF_i&quot; alt=&quot;\sum_{i=1}^{n} p^{BC}_i + p^T_i + \frac{1}{2} p^F_i&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%5Ctext%7Bsubject%20to%3A%7D&quot; alt=&quot;\text{subject to:}&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/b_i*f_i%3C4%2Bp%5ET_i%20%5Cqquad%20%5Cforall%20%5C%20i&quot; alt=&quot;b_i*f_i&amp;lt;4+p^T_i \qquad \forall \ i&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%7Cf_i-s_i%7C%20-%20(t_i*M)%20%5Cleq%20p_i%5EF%20%5Cqquad%20%5Cforall%20%5C%20i&quot; alt=&quot;|f_i-s_i| - (t_i*M) \leq p_i^F \qquad \forall \ i&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/-(s_i-s_%7Bi%2B1%7D)(f_i-f_%7Bi%2B1%7D)%5Cleq%20M*p%5E%7BBC%7D_i%20%5Cqquad%20%5Cforall%20%5C%20i%20%3C%20n-1&quot; alt=&quot;-(s_i-s_{i+1})(f_i-f_{i+1})\leq M*p^{BC}_i \qquad \forall \ i &amp;lt; n-1&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/f_i%20%2B%20f_%7Bi%2B1%7D%20%2B%20f_%7Bi%2B2%7D%20%5Cgeq%205%20%5Cqquad%20%5Cforall%20%5C%20i%20%3C%20n-2&quot; alt=&quot;f_i + f_{i+1} + f_{i+2} \geq 5 \qquad \forall \ i &amp;lt; n-2&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%7Cf_i-f_%7Bi%2B1%7D%7C%20%5Cgeq%201%20%5Cqquad%20%5Cforall%20%5C%20i%20%3C%20n-1&quot; alt=&quot;|f_i-f_{i+1}| \geq 1 \qquad \forall \ i &amp;lt; n-1&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;An explanation of this model as well as other information about this project can be found in &lt;a href=&quot;/assets/docs/RHFingerings.pdf&quot;&gt;this paper.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We implemented the model with Gurobi Optimizer and tested it on some selected measures of “La Catedral”. Below are some examples of the results. The f &lt;sub&gt;i&lt;/sub&gt; correspond to the finger assignment of each note as returned by our model and the f &lt;sub&gt;i&lt;/sub&gt;* correspond to the optimal finger assignment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/measure1.png&quot; alt=&quot;drawing&quot; width=&quot;200&quot; style=&quot;float: center;&quot; /&gt; 
&lt;img src=&quot;/assets/img/measure2.png&quot; alt=&quot;drawing&quot; width=&quot;200&quot; style=&quot;float: center;&quot; /&gt; 
&lt;img src=&quot;/assets/img/measure3.png&quot; alt=&quot;drawing&quot; width=&quot;160&quot; style=&quot;float: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/mskarha/rhfingerings&quot;&gt;here&lt;/a&gt; to access the Github repository of the Python code used to generate the .lp file in order to use our model on your own music.&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Integer Programming for Optimal Right Hand Guitar Fingerings</summary></entry><entry><title type="html">Laser Interferometry for Loudspeaker Characterization</title><link href="http://localhost:4000/20171203/laser-interferometry" rel="alternate" type="text/html" title="Laser Interferometry for Loudspeaker Characterization" /><published>2017-12-03T00:00:00-05:00</published><updated>2017-12-03T00:00:00-05:00</updated><id>http://localhost:4000/20171203/laser-interferometry</id><content type="html" xml:base="http://localhost:4000/20171203/laser-interferometry">&lt;h2 id=&quot;laser-interferometry-for-loudspeaker-characterization&quot;&gt;Laser Interferometry for Loudspeaker Characterization&lt;/h2&gt;

&lt;p&gt;In this experiment, I set up Michelson interferometer to measure the frequency response of a commercial loudspeaker to high precision.&lt;/p&gt;

&lt;p&gt;In a Michelson interferometer, a light source (in our case, a He-Ne laser) is split into two arms with a beam splitter. Each of those beams is reflected back towards the beamsplitter via a mirror, which then combines their amplitudes using the superposition principle. The resulting interference pattern is then picked up by a photoelectric detector. Because of the small wavelength of our light, this technique can be used to measure extremely small displacements, in accordance with the path length difference between the two arms.&lt;/p&gt;

&lt;p&gt;I decided to mount one of the mirrors on the cone of my &lt;a href=&quot;http://www.jblpro.com/www/products/recording-broadcast/3-series/lsr305#.W4wu8NhKiRs&quot;&gt;JBL LSR305&lt;/a&gt; studio monitor that I use for mixing music. One of the main principles behind studio monitors is that the frequency response of the speakers must be as flat as possible so as to provide the most pure representation of whatever sound is being played. This is crucial for audio engineers who are dealing with very minute details in a recording. Typically, manufacturers of home stereo systems and other speakers go at great lengths to enhance audio quality by boosting certain bass, mid, and high frequencies. This renders normal speakers almost useless for audio production.&lt;/p&gt;

&lt;p&gt;Using this experimental setup, I was able to verify the flatness of the JBL LSR305 studio monitor by sweeping through a large range of frequencies and determining the cone displacement based on the interference pattern. &lt;a href=&quot;/assets/docs/Laser interferometric characterization for a vibrating speaker system good.pdf&quot;&gt;Here&lt;/a&gt; is a paper I wrote about the experimental process. Check out this image of the experimental setup:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/michelson.jpg&quot; alt=&quot;drawing&quot; width=&quot;450&quot; style=&quot;float: center;&quot; /&gt;&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;My proudest snapchat moment ever was when I set up a laser interferometer in physics lab to measure speaker cone displacement then played technotronic&amp;#39;s pump up the jam and took data with an oscilloscope built by &lt;a href=&quot;https://twitter.com/tektronix?ref_src=twsrc%5Etfw&quot;&gt;@tektronix&lt;/a&gt; &lt;a href=&quot;https://t.co/Tu06GeEF6M&quot;&gt;pic.twitter.com/Tu06GeEF6M&lt;/a&gt;&lt;/p&gt;&amp;mdash; skorched earth policy (@skarhaha56) &lt;a href=&quot;https://twitter.com/skarhaha56/status/1012424839993413632?ref_src=twsrc%5Etfw&quot;&gt;June 28, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Laser Interferometry for Loudspeaker Characterization</summary></entry><entry><title type="html">Chaotic Double Pendulum Synthesizer</title><link href="http://localhost:4000/20170831/chaotic-pendulum" rel="alternate" type="text/html" title="Chaotic Double Pendulum Synthesizer" /><published>2017-08-31T00:00:00-04:00</published><updated>2017-08-31T00:00:00-04:00</updated><id>http://localhost:4000/20170831/chaotic-pendulum</id><content type="html" xml:base="http://localhost:4000/20170831/chaotic-pendulum">&lt;h2 id=&quot;chaotic-double-pendulum-synthesizer&quot;&gt;Chaotic Double Pendulum Synthesizer&lt;/h2&gt;

&lt;p&gt;The Chaotic Double Pendulum Synthesizer is an attempt to find musicality in the laws of physics that govern our world.&lt;/p&gt;

&lt;p&gt;The double pendulum is a simple idea, but complex mathematically. The basic idea of a chaotic double pendulum is that it is a simple physical system (a pendulum attached to the end of another pendulum) that exhibits rich dynamic behavior (much more interesting motion that one would expect) with a strong sensitivity to initial conditions. The motion is often described by physicists as unexpectedly mesmerizing.&lt;/p&gt;

&lt;p&gt;The goal of this project is to capture that mesmerizing motion with electroacoustic music using Max/MSP motion tracking and sound processing. By feeding data about the position, velocity, and acceleration of the pendulums into a series of oscillators, we can “listen” to the chaos that governs some of the laws of physics of our universe.&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;http://scienceworld.wolfram.com/physics/DoublePendulum.html&quot;&gt;here&lt;/a&gt; to learn more about some of the math behind coupled pendula. Additionally, click &lt;a href=&quot;https://www.youtube.com/watch?v=AwT0k09w-jw&quot;&gt;here&lt;/a&gt; to watch a demo of a chaotic double pendulum.&lt;/p&gt;

&lt;p&gt;Click the image below to watch a brief demo of the synthesizer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=Op3IMovg4CM&quot;&gt;&lt;img src=&quot;http://img.youtube.com/vi/Op3IMovg4CM/0.jpg&quot; alt=&quot;synth&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mskarha/Chaotic-Double-Pendulum-Synthesizer&quot;&gt;Github Repository&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Chaotic Double Pendulum Synthesizer</summary></entry></feed>