<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-12-10T17:54:14-05:00</updated><id>http://localhost:4000/</id><title type="html">Matt Skarha</title><subtitle>Your description here</subtitle><entry><title type="html">Spectral Descriptors as Control Objects for Mass-Interaction Physical Modeling</title><link href="http://localhost:4000/20201204/mass-interaction" rel="alternate" type="text/html" title="Spectral Descriptors as Control Objects for Mass-Interaction Physical Modeling" /><published>2020-12-04T00:00:00-05:00</published><updated>2020-12-04T00:00:00-05:00</updated><id>http://localhost:4000/20201204/mass-interaction</id><content type="html" xml:base="http://localhost:4000/20201204/mass-interaction">&lt;h1 id=&quot;spectral-descriptors-as-control-objects-for-mass-interaction-physical-modeling-sound-synthesis&quot;&gt;Spectral Descriptors as Control Objects for Mass-Interaction Physical Modeling Sound Synthesis&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;200&quot; src=&quot;/assets/img/beam.png&quot; /&gt;&lt;br /&gt;
  &lt;em&gt;Figure 1. Physical model of a beam with 86 masses and 512 interactions, &lt;br /&gt;excited by a plucking mechanism. Taken from [9].&lt;/em&gt;
&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Walking down the street with my headphones on, I observe that I am
listening to both continuous-time signals (my footsteps, the cars
passing by) and digitally-processed discrete-time signals (lofi house
mix on SoundCloud through my headphones). However, my brain more-or-less
perceives them as both continuous-time, in a way blurring the lines
between the analog and digital worlds. At the center of this blurring
is, of course, signal processing. Wielding this idea has numerous
applications, particularly for digital sound synthesis for music
creation. If we can understand how to model mechanical and acoustical
systems that are governed by physical laws using discrete-time and
discrete-space mathematical formalisms, we can simulate these systems
with a control that might not be feasible in the analog realm. This is
then the task of physical modeling sound synthesis.&lt;/p&gt;

&lt;h1 id=&quot;physical-modeling-for-sound-synthesis&quot;&gt;Physical Modeling for Sound Synthesis&lt;/h1&gt;

&lt;p&gt;Within the realm of computer music, a number of formalisms for physical
modeling have been proposed over the past forty years. A few are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;finite difference schemes&lt;/li&gt;
  &lt;li&gt;lumped models&lt;/li&gt;
  &lt;li&gt;digital waveguides&lt;/li&gt;
  &lt;li&gt;modal synthesis&lt;/li&gt;
  &lt;li&gt;finite element modeling&lt;/li&gt;
  &lt;li&gt;state-space techniques&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Certainly each of these formalisms have their own pros and cons and will
be suitable or unsuitable depending on the system. Often combinations of
techniques can be used to achieve a more precise model. In 2003,
Castagne and Cadoz proposed 10 general criteria for evaluating physical
modeling techniques oriented to music creation [1]:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How efficient is the algorithm?&lt;/li&gt;
  &lt;li&gt;How faithful are the synthesized sounds?&lt;/li&gt;
  &lt;li&gt;How diverse are the categories of instruments that can be modelled?&lt;/li&gt;
  &lt;li&gt;Is the scheme exclusively dedicated to sound synthesis or more
general?&lt;/li&gt;
  &lt;li&gt;How robust is sound plausability?&lt;/li&gt;
  &lt;li&gt;How modular is the technique?&lt;/li&gt;
  &lt;li&gt;How intuitive and effective is the associated mental model?&lt;/li&gt;
  &lt;li&gt;How deep is the modeling process enabled by the scheme?&lt;/li&gt;
  &lt;li&gt;Do generation algorithms exist?&lt;/li&gt;
  &lt;li&gt;Is there a friendly musician-oriented environment for using the
scheme?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;While it is not the task of this project to compare and contrast
modeling schemes, I think these are good criteria to keep in mind when
considering this project.&lt;/p&gt;

&lt;h1 id=&quot;mass-interaction-physical-modeling&quot;&gt;Mass-Interaction Physical Modeling&lt;/h1&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Mass-Interaction Physical Modeling is one of the oldest techniques for
digital sound synthesis via physical modeling. It was pioneered
beginning in 1978 by the CORDIS-ANIMA system developed at ACROE in
Grenoble, France [2]. The basic idea is to represent physical systems
in the form of lumped networks composed of two main components: mass
objects and interaction objects. Mass objects represent material points
in a given space with some inertial behavior while interaction objects
represent a specific type of coupling between mass objects (i.e.
visco-elastic, collision, non-linear). By doing this, we avoid the need
to explicitly define a mathematical model such as a partial differential
equation system with boundary conditions (as is needed in finite
difference methods).&lt;/p&gt;

&lt;p&gt;Within the past few years, there has been somewhat of a renewed interest
in mass-interaction physical modeling, driven by the release of a
Max/MSP package mi-gen~ [3]
(&lt;a href=&quot;#https://github.com/mi-creative/mi-gen&quot;&gt;github&lt;/a&gt;) and a Faust [4]
(&lt;a href=&quot;#https://github.com/rmichon/mi_faust&quot;&gt;github&lt;/a&gt;) library. These tools
have certainly improved the accessibility of MI-modeling thanks to the
inclusion of a model scriptor that translates a high level model into
lower level code.&lt;/p&gt;

&lt;h2 id=&quot;basics&quot;&gt;Basics&lt;/h2&gt;

&lt;p&gt;In order to model the physics of masses and interactions in
discrete-time, we will apply a second order central difference scheme to
Newton’s second law. For a point mass, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f = ma = m\frac{d^2x}{dt^2}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is the force applied to the mass, &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is its inertia, &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; is
its acceleration, and &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is its position.&lt;/p&gt;

&lt;p&gt;Using a second order central difference scheme with sampling period
&lt;script type=&quot;math/tex&quot;&gt;\Delta T&lt;/script&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(t) = m\frac{x(t+\Delta T) - 2x(t) + x(t-\Delta T)}{\Delta T^2}.&lt;/script&gt;

&lt;p&gt;To discretize, we simply convert the sample period into a sample and
collect the continuous-time parameters into a discrete-time inertial
constant &lt;script type=&quot;math/tex&quot;&gt;M = \frac{m}{\Delta T^2}.&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f[n] = M(x[n+1] - 2x[n] + x[n-1]).&lt;/script&gt;

&lt;p&gt;Rearranging terms, we can achieve a difference equation that describes
the discrete-time position of a mass as a result of the external forces
being applied:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x[n+1] = 2x[n] - x[n-1] + \frac{f[n]}{M}.&lt;/script&gt;

&lt;p&gt;A very common object in mass-interaction modeling is a visco-elastic
spring connecting two masses. Consider a spring with stiffness parameter
&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; and damping parameter &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. Hooke’s Law tells us that the force
needed to extend or compress a spring by some distance &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; scales
linearly with respect to that distance. In the case of a spring
connecting masses as shown in Figure 1, we have&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;100&quot; src=&quot;\assets\img\spring.png&quot; /&gt;&lt;br /&gt;
  &lt;em&gt;Figure 1. A visco-elastic spring connecting two masses, m_1 and m_2 at
positions x_1 and x_2, respectively.&lt;/em&gt;
&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{s,1\rightarrow 2} = -k(x_2 - x_1)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt; are the positions of the two masses &lt;script type=&quot;math/tex&quot;&gt;m_1&lt;/script&gt; and
&lt;script type=&quot;math/tex&quot;&gt;m_2&lt;/script&gt;, respectively. In discrete-time, this is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{s,1\rightarrow 2}[n] = -K(x_2[n] - x_1[n])&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;K = k&lt;/script&gt; is the discrete-time stiffness parameter.&lt;/p&gt;

&lt;p&gt;If we approximate damping to the first-order, we have the following
relationship describing the force from &lt;script type=&quot;math/tex&quot;&gt;m_1&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;m_2&lt;/script&gt; as a result of
the damper:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{d,1\rightarrow 2} = -z\frac{d(x_2 - x_1)}{dt}.&lt;/script&gt;

&lt;p&gt;Using a first-order backward difference scheme to discretize the
derivative, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{d,1\rightarrow 2} = -z\frac{(x_2(t) - x_1(t)) - (x_2(t-\Delta T) - x_1(t-\Delta T))}{\Delta T},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Delta T&lt;/script&gt; is the sampling period. In discrete-time, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{d,1\rightarrow 2}[n] = -Z[(x_2[n] - x_2[n-1]) - (x_1[n] - x_1[n-1])].&lt;/script&gt;

&lt;p&gt;Thus, we have computed the discrete-time external forces for a spring
connecting two masses. More common, however, is the world-reknown linear
harmonic damped oscillator (mass on a spring) where one end of the
spring is now fixed to a point, as shown in Figure 2. Writing the sum of
the forces on mass &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; as a result of the spring and the damper, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{tot, m}[n] = f_{s,m}[n] + f_{d,m}[n]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{tot, m}[n] = -Kx[n] - Zx[n] + Zx[n-1].&lt;/script&gt;

&lt;p&gt;We can now substitute this expression into the discrete-time Newton’s
second law from above and rearrange terms to achieve a difference
equation that describes the motion of a harmonic oscillator.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;100&quot; src=&quot;\assets\img\springmass.png&quot; /&gt;&lt;br /&gt;
  &lt;em&gt;Figure 2. A visco-elastic spring connecting a mass and a fixed point.&lt;/em&gt;
&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x[n+1] = \left( 2 - \frac{K+Z}{M}\right)x[n] + \left(\frac{Z}{M} -1 \right ) x[n-1]&lt;/script&gt;

&lt;p&gt;Notice that the position of the mass at time &lt;script type=&quot;math/tex&quot;&gt;n+1&lt;/script&gt; simply depends on a
linear combination of the position of the mass at time &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;n-1&lt;/script&gt;,
where the coefficients are simply terms in the stiffness parameter &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;,
the damping parameter &lt;script type=&quot;math/tex&quot;&gt;Z,&lt;/script&gt; and the inertial parameter &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;. In practice,
these parameters are normalized to 1. Although they are functions of the
sample rate, they do provide a direct view of stability conditions for
the system. In order to stay in an oscillatory regime, we must have
&lt;script type=&quot;math/tex&quot;&gt;4M &gt; K + 2Z&lt;/script&gt;. A derivation of stability conditions for the linear
harmonic oscillator can be found in the appendix of [3].&lt;/p&gt;

&lt;h2 id=&quot;building-topologies&quot;&gt;Building Topologies&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;250&quot; src=&quot;\assets\img\triangle.png&quot; /&gt;&lt;br /&gt;
  &lt;em&gt;Figure 3. Physical model of a triangle. Taken from
[4].&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;Based on this formalism, modeling with MI involves building a
geometrical model by positioning and connecting masses together through
interaction objects, and by specifying the parameters and initial
conditions for each one. Figure 3 shows a basic physical model of a
triangle (the percussion instrument). Three masses are joined by
dampened springs with one of the masses fixed to a point. The system is
then excited by a pluck at the input module and the output is taken from
one of the masses. Because sound is essentially just air being pushed,
we can listen to the motion of a given mass as the output of our system.&lt;/p&gt;

&lt;p&gt;One main advantage arises when comparing MI modeling with other physical
modeling paradigms: It introduces the ability to build and excite
virtual mechanical constructions that are not bounded by realism. We
know from more traditional musical acoustics research that many
nonlinear behaviors exist in acoustic musical instruments which
contribute to their unique and interesting timbres. It is often the task
of other physical modeling techniques to model these nonlinearities for
a given system with the goal of achieving a more realistic sound
synthesis. With MI modeling, we are &quot;atomically&quot; building structures
grounded in Newton’s second law, meaning nonlinear behavior will arise
naturally if a system is excited properly and maintains numerical
stability. If we can model and excite the right structure, theoretically
we can discover new interesting behaviors and sounds for use in an
artistic process.&lt;/p&gt;

&lt;p&gt;One main disadvantage, however, is the task at hand. It is difficult to
develop an intuition of how to model structures to achieve a certain
timbre or behavior, especially when there are three parameters to adjust
for every object. The goal of this project, then, is to simplify this
process by introducing higher level (timbral) control objects to build
MI geometries.&lt;/p&gt;

&lt;h1 id=&quot;towards-higher-level-control-of-mi-systems&quot;&gt;Towards Higher Level Control of MI Systems&lt;/h1&gt;

&lt;p&gt;This project attempts to introduce higher level control objects for
mass-interaction physical models. This was done by first assessing what
a set optimal control parameters would look like in this context. Next,
a series of experiments were conducted to build models for the
relationships between existing parameters and optimal parameters. And
finally, a Max patch was developed with these controls as input to
determine if it is a more intuitive approach to MI modeling.&lt;/p&gt;

&lt;h2 id=&quot;optimal-control-parameters&quot;&gt;Optimal Control Parameters&lt;/h2&gt;

&lt;p&gt;If an artist wants to use mass-interaction modeling as a sound synthesis
paradigm in their creative process, what parameters would they be
interested in controlling? Considering the victories of previous
synthesizers, I contend pitch and decay rate would be suitable as a
start.&lt;/p&gt;

&lt;p&gt;As for timbral control, we can use spectral descriptors to build a
control object. Spectral descriptors are a set of perceptually-relevant
algorithms that can be computed on any audio signal to learn information
about that signal. Some examples are spectral centroid, decrease,
kurtosis, and spread. Sound events are analyzed in terms of various
input representations including the short-term Fourier transform,
harmonic sinusoidal components, and an auditory model based on the
equivalent rectangular bandwidth concept. A number of audio descriptors
are then derived from each of these representations to capture temporal,
spectral, spectrotemporal, and energetic properties of the sound events
[5].&lt;/p&gt;

&lt;p&gt;There are often correlations between spectral descriptors and perceptual
attributes. For example, spectral centroid is known to be correlated
with brightness. I contend that we can exploit this idea to obtain
higher level control over mass-interaction physical models.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;250&quot; src=&quot;\assets\img\fundfit.png&quot; /&gt;&lt;br /&gt;
  &lt;em&gt;Figure 4.Inertial parameter M vs. fundamental frequency. A least-squares fit
indicates a first-order power law
relationship.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;To determine the relationship between the inertial parameter and the
pitch, an experiment was carried out by varying the inertial parameter
&lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; and estimating the fundamental frequency, as shown in Figure 4. A 50
mass string was used as the test structure. The data was fit to a
first-order power law of the form &lt;script type=&quot;math/tex&quot;&gt;f(x) = ax^b&lt;/script&gt; and a 0.9995 R-square
value was achieved. Thus, to control the inertial parameter with pitch,
an inverse function &lt;script type=&quot;math/tex&quot;&gt;f^{-1}(x)&lt;/script&gt; is taken and solved for &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M = af_0^b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_0 = \sqrt[b]{M/a},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; is the discrete-time inertial parameter, &lt;script type=&quot;math/tex&quot;&gt;f_0&lt;/script&gt; is the
fundamental frequency in Hz, &lt;script type=&quot;math/tex&quot;&gt;a = 113.4&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;b = -0.4982&lt;/script&gt;. It is
important to note that while the first order power law relationship
holds across different structures, the coefficients of the model do not
and can therefore not be generalizable. This drastically increases the
difficulty for pitch control via inertial parameter modulation and it
would likely be easier to use known pitch shifting techniques.&lt;/p&gt;

&lt;p&gt;Next, an experiment was carried out to determine the relationship
between the inertial parameter and the decay time, as shown in Figure 5.
This indicated a linear relationship between the two variables and a
control object was constructed accordingly. Again, while the fit
coefficients are not generalizable for any given structure, the linear
relationship holds.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;250&quot; src=&quot;\assets\img\decayfit.png&quot; /&gt;&lt;br /&gt;
  &lt;em&gt;Figure 5. Inertial parameter M vs. decay time (ms). A least-squares fit
indicates a linear relationship.&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;To determine what a timbral control object would look like, a number of
experiments were carried out using spectral descriptors to try to
determine the relationship between MI geometry and timbre. First,
several spectral descriptors were measured against the number of masses
in an ideal string to determine the timbral effect of masses. Some of
the results are shown in Figure 6. Adding masses to a string therefore
decreases centroid, increases spectral decrease, increases kurtosis, and
decreases slope.&lt;/p&gt;

&lt;p&gt;Then, six mid-level macro objects were constructed using the Mass
Interaction Model Scripter (MIMS) provided in the mi-gen  Max/MSP
toolbox: string (ideal string fixed at both ends), stiff string (a
string accounting for 2nd order stiffness, fixed at both ends), a chain
(string but not fixed at end), mesh (2d string with open boundary
conditions), closedMesh (entirely fixed boundary conditions), and
cornerMesh (fixed points at each of the four corners). Then, the
parameters were tuned so that all objects had the same fundamental
frequency and damping (it doesn’t make sense to compare timbres of
objects of different pitches/decays). The objects were measured against
spectral descriptors (centroid, decrease, kurtosis, spread, skewness,
rolloff) to try to determine their effect on the timbre. Some of the
results are shown in Figure 7. The main takeaway from these experiments
was that not fixing points has the effect of boosting low frequencies
(as in the case of the chain).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;450&quot; src=&quot;\assets\img\sda.png&quot; /&gt;&lt;br /&gt;
  &lt;em&gt;Figure 6. Some results of experiment testing timbral effects of number of masses
on an ideal string.&lt;/em&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;250&quot; src=&quot;\assets\img\centroidec.png&quot; /&gt;&lt;br /&gt;
  &lt;em&gt;Figure 7. Some results of experiment testing timbral effects of various
mid-level structures.&lt;/em&gt;
&lt;/p&gt;

&lt;h2 id=&quot;max-patch&quot;&gt;Max Patch&lt;/h2&gt;

&lt;p&gt;A Max patch was constructed with a novel &quot;timbral control&quot; object
based on the results of the experiments in the previous section. The
object is based off of Max’s &lt;em&gt;pattrstorage&lt;/em&gt; object while allows for
linear interpolation between presets. Six timbral &quot;modes&quot; are
contained as presets in this object, between which the user can
interpolate. The modes were determined by varying the gain of each of
the six macro objects described earlier while holding the other five
constant and determining their effect on six spectral descriptors
(centroid, kurtosis, decrease, skewness, rolloff). If a spectral
descriptor quantity reached a maximum as the gains were varied, it was
determined to be a timbral &quot;mode&quot; and was added to the object.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Mass-Interaction Physical Modeling is a physical modeling sound
synthesis technique performed by representing physical systems in the
form of lumped networks composed of two main components: mass objects
and interaction objects. Modeling with MI involves building a
geometrical model by positioning and connecting masses together through
interaction objects, and by specifying the parameters and initial
conditions for each one. However, it is difficult to achieve intuition
over how to build physical models since the objects are so low-level.
This project sought to create higher level timbral control objects by
experimenting with spectral descriptors across various topologies.&lt;/p&gt;

&lt;p&gt;While I have made some progress towards higher level control of MI
models, there is still far more to be done. The task ended up being far
more difficult than I expected and would require a much deeper
understanding in order to implement such an object in a synthesizer.
Nonetheless, I believe MI modeling has endless potential as a physical
modeling technique and I look forward to continue studying it in the
future.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;p&gt;[1] N. Castagne, C. Cadoz, “10 Criteria for evaluating physical modelling
schemes for music creation”, Proceeding of the Digital Audio Effects
Conference DAFX03,London, UK, 2003&lt;/p&gt;

&lt;p&gt;[2] Cadoz, C., Luciani, A., &amp;amp; Florens, J. L. (1993). CORDIS-ANIMA: A
Modeling and Simulation System for Sound and Image Synthesis: The
General Formalism. Computer Music Journal, 17(1), 19-29.
doi:10.2307/3680567&lt;/p&gt;

&lt;p&gt;[3] Castagné, N., &amp;amp; Cadoz, C. (2002). GENESIS : a Friendly Musician-Oriented
Environment for Mass-Interaction Physical Modeling. International
Computer Music Conference Proceedings.&lt;/p&gt;

&lt;p&gt;[4] Leonard, J., &amp;amp; Villeneuve, J. (2019, 2019-05-28). mi-gen : An Efficient
and Accessible Mass-Interaction Sound Synthesis Toolbox. Paper presented
at the SMC 2019 - 16th Sound &amp;amp; Music Computing Conference, Malaga,
Spain.&lt;/p&gt;

&lt;p&gt;[5] Leonard, J., Villeneuve, J., Michon, R., &amp;amp; Orlarey, Y. (2019).
Formalizing Mass-Interaction Physical Modeling in Faust.&lt;/p&gt;

&lt;p&gt;[6] Peeters G, Giordano BL, Susini P, Misdariis N, McAdams S. The Timbre
Toolbox: extracting audio descriptors from musical signals. J Acoust Soc
Am. 2011 Nov;130(5):2902-16. doi: 10.1121/1.3642604. PMID: 22087919.&lt;/p&gt;

&lt;p&gt;[7] Kontogeorgakopoulos, A., &amp;amp; Claude, C. (2007). Cordis Anima Physical
Modeling and Simulation System Analysis.&lt;/p&gt;

&lt;p&gt;[8] Leonard, J., &amp;amp; Cadoz, C. (2015). Physical Modelling Concepts for a
Collection of Multisensory Virtual Musical Instruments. Paper presented
at the Proceedings of the international conference on New Interfaces for
Musical Expression, Baton Rouge, Louisiana, USA.&lt;/p&gt;

&lt;p&gt;[9] Jerome Villeneuve, James Leonard. Mass-Interaction Physical Models for Sound ans Multi-Sensory Creation : Starting Anew. SMC 2019 - 16th Sound &amp;amp; Music Computing Conference, May 2019, Malaga, Spain.&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Spectral Descriptors as Control Objects for Mass-Interaction Physical Modeling Sound Synthesis Figure 1. Physical model of a beam with 86 masses and 512 interactions, excited by a plucking mechanism. Taken from [9].</summary></entry><entry><title type="html">eggDJ: A Portable Real-Time Music Augmentation System</title><link href="http://localhost:4000/20191210/lorem-ipsum" rel="alternate" type="text/html" title="eggDJ: A Portable Real-Time Music Augmentation System" /><published>2019-12-10T00:00:00-05:00</published><updated>2019-12-10T00:00:00-05:00</updated><id>http://localhost:4000/20191210/lorem-ipsum</id><content type="html" xml:base="http://localhost:4000/20191210/lorem-ipsum">&lt;h1 id=&quot;eggdj-a-portable-real-time-music-augmentation-system&quot;&gt;eggDJ: A Portable Real-Time Music Augmentation System&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;200&quot; src=&quot;/assets/img/eggDJ.jpg&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;From time to time, I catch myself energetically tapping my fingers to the beat of my music while on the bus and the metro. For a brief moment, I become so captivated by the groove in my headphones that I completely lose awareness of my surroundings. I find myself attempting to match the kick/snare pattern with my index and middle fingers as well as embellish the drums with rhythms of my own.&lt;/p&gt;

&lt;p&gt;This project is an attempt to bring those moments to life.&lt;/p&gt;

&lt;h2 id=&quot;live-augmentation-as-musical-practice&quot;&gt;Live Augmentation as Musical Practice&lt;/h2&gt;

&lt;p&gt;I would like to introduce the notion of live augmentation as a musical practice. Live augmentation, in this context (not to be confused with the music theory term of the same name describing the lengthening of a note), simply refers to adding your own flair to an existing piece of recorded music in real-time. This can be anything from layering vocal samples on top of a lofi SoundCloud house mix to practicing your jazz guitar comping by playing along with an famous jazz recording. The benefits of such practice when compared with the traditional practice of acoustic instruments are threefold: it is often a much more accessible approach to music, it can have interesting pedagogical effects, and it can breathe new life into an existing piece of music, opening up a new realm of creativity.&lt;/p&gt;

&lt;p&gt;Take the example of the guitarist comping along with a Miles Davis record. It is not necessarily an easy task to get, for example, a trumpet player, a bassist, and a drummer in a particular room at a particular time a) that you get along with, b) that are at a similar skill level as you, c) have similar musical goals as you, and d) without being too loud as to disturb others. Not to mention all the equipment hauling and level mixing that needs to be done. By simply playing along with a favorite record, the guitarist can often accomplish many of their musical goals much more easily.&lt;/p&gt;

&lt;p&gt;Additionally, many researchers in the field of Accessible Digital Musical Instruments, or ADMIs, (that seek to build musical control interfaces for persons with physical or mental disabilities) take the “augmentation” approach due to its low barrier to entry and retention of musical nuance [1].&lt;/p&gt;

&lt;p&gt;Live augmentation can also be a very pedagogical tool. If we play along with a recording that is deemed to “have more merit” within a particular tradition of music, we can often learn more easily by engaging directly with the music as compared to passive listening. The obvious example is the jazz guitarist emulating the comping technique of say, Freddie Green, while playing along with a Count Basie record, in an attempt to improve their comping skills. By inserting their own musical creativity into the piece, the guitarist is also, by definition, creating a new piece of music that has infinite potential for innovation.&lt;/p&gt;

&lt;h2 id=&quot;design-goals&quot;&gt;Design Goals&lt;/h2&gt;

&lt;p&gt;For this project, I wanted to implement a real-time music augmentation system that I can use on my way to class. This portability constraint made the project much more interesting than traditional music technology hardware, that often sits in a studio and requires a desktop computer for interaction.&lt;/p&gt;

&lt;p&gt;I had the following goals in mind while I designed eggDJ:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;it had to be able to play music from any streaming service (e.g. Spotify, SoundCloud, YouTube)&lt;/li&gt;
  &lt;li&gt;it had to have the ability to layer any digital sound with a basic sampler mechanism&lt;/li&gt;
  &lt;li&gt;the entire system should fit in a jacket pocket wirelessly&lt;/li&gt;
  &lt;li&gt;it should have zero visual aspect (only hearing and touch)&lt;/li&gt;
  &lt;li&gt;no one should be able to tell what you are doing (don’t wanna look weird on the bus)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hardware&quot;&gt;Hardware&lt;/h2&gt;

&lt;p&gt;An initial reaction to these design goals might be to simply build an Android/iPhone app where one can trigger samples by tapping the screen. While this would be fairly straightforward, it doesn’t accomplish all of the goals well because it would be difficult to interact with while not looking at it. 
&lt;img height=&quot;100&quot; align=&quot;right&quot; src=&quot;/assets/img/pi.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I decided to use a Raspberry Pi 3 with an &lt;a href=&quot;http://www.audioinjector.net/rpi-zero&quot;&gt;Audio Injector Zero Shield&lt;/a&gt; to implement the hardware side. It is notoriously difficult to get audio into the Pi (a simple Google search will confirm this, PureData’s website has the &lt;a href=&quot;https://puredata.info/docs/raspberry-pi&quot;&gt;best list of working soundcards&lt;/a&gt; that I was able to find). Even with the Audio Injector Zero, it still took several hours to figure out how to get audio in and out using the sound card. 
&lt;img height=&quot;100&quot; align=&quot;right&quot; src=&quot;/assets/img/zero.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to trigger the samples, I just decided to use 5 simple pushbuttons, one for each finger. The simple control interface makes the eggDJ very intuitive with a low entry-fee.&lt;/p&gt;

&lt;p&gt;Unfortunately, when I tried to use the Audio Injector Shield (which takes over all 40 of the Pi’s GPIO pins) in tandem with buttons attached to the GPIO pins, the Pi crashed. Thus, in order to get the pushbutton data into the Pi, I used a Teensy 3.2 interfaced via a micro-USB cable. The Teensy 3.2 has pullup resistors built-in to all 21 digital I/O pins, making it preferable compared to an Arduino Uno.&lt;/p&gt;

&lt;h2 id=&quot;software&quot;&gt;Software&lt;/h2&gt;

&lt;p&gt;On the software side of things, the most straightforward way to trigger samples over an audio input on a Raspberry Pi is using Pure Data (MSP’s Vanilla version). My patch (shown below) implements a simple sample selection mechanism, where the user cycles through an arbitrary number (in this case, ten) of samples on a given channel (finger) by pressing the thumb button.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;300&quot; src=&quot;/assets/img/tekmit.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;My Arduino code (below) simply writes which button was pushed to a serial stream which is read by Pd’s comport object. More details are in the code comments and ReadMe file.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://create.arduino.cc/editor/mskarha/742d57c9-5db0-4dcd-8c61-98dfa7ed13e3/preview?embed&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=idyqRyKqyLU&quot;&gt;https://www.youtube.com/watch?v=idyqRyKqyLU&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Frid, Emma. “Accessible Digital Musical Instruments - a Survey of Inclusive Instruments Presented at the NIME, SMC and ICMC Conferences.” ICMC (2018).&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">eggDJ: A Portable Real-Time Music Augmentation System</summary></entry><entry><title type="html">Cooperative Listening Devices with Distributed and Wearable Microphone Arrays</title><link href="http://localhost:4000/20190809/micarrays" rel="alternate" type="text/html" title="Cooperative Listening Devices with Distributed and Wearable Microphone Arrays" /><published>2019-08-09T00:00:00-04:00</published><updated>2019-08-09T00:00:00-04:00</updated><id>http://localhost:4000/20190809/micarrays</id><content type="html" xml:base="http://localhost:4000/20190809/micarrays">&lt;h1 id=&quot;cooperative-listening-devices-with-distributed-and-wearable-microphone-arrays&quot;&gt;Cooperative Listening Devices with Distributed and Wearable Microphone Arrays&lt;/h1&gt;

&lt;p&gt;I was fortunate enough to spend two summers (‘18 &amp;amp; ‘19) as a research assistant in the lab of &lt;a href=&quot;http://ryanmcorey.com/&quot;&gt;Ryan Corey, PhD&lt;/a&gt;. When I was working with him, he was a doctoral candidate in Electrical and Computer Engineering at the University of Illinois at Urbana-Champaign. This post summarizes our work.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;300&quot; src=&quot;/assets/img/room1.jpg&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Hearing aids don’t work well in noisy, reverberant environments where lots of people are talking at once (e.g. a crowded bar, conference poster session, etc.). Ryan, a hearing aid user himself, devoted his entire Ph.D. to improving the performance of hearing aids in noisy environments. In doing this, he also helped to develop what he calls ‘augmented listening (AL)’ technologies. You can read more about augmented listening technologies &lt;a href=&quot;https://publish.illinois.edu/augmentedlistening/what-is-augmented-listening/&quot;&gt;here&lt;/a&gt;, but in general, they can be grouped into four categories of products:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;traditional hearing aids&lt;/li&gt;
  &lt;li&gt;personal sound amplification products (&lt;a href=&quot;https://en.wikipedia.org/wiki/Personal_sound_amplification_product&quot;&gt;PSAPs&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;smart headphones, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Hearables&quot;&gt;“hearables”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;virtual, augmented, and mixed reality headsets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Much of Ryan’s work, where the primarily application is traditional hearing aids, can be readily extended to these other AL technologies.&lt;/p&gt;

&lt;h2 id=&quot;real-world-sound-de-mixing-and-re-mixing&quot;&gt;Real-World Sound De-mixing and Re-mixing&lt;/h2&gt;

&lt;p&gt;In an ideal world, a hearing aid user could turn up the volume of a particular sound source (a person they are conversing with) and turn down the volume of background noise (mechanical noise, other people talking, loud music, etc.). A perfect AL system would work like a mixing board in a recording studio: each sound source would have its own slider, and the user — or an algorithm — would decide how much of each source should end up in the final mix.&lt;/p&gt;

&lt;p&gt;&lt;img height=&quot;100&quot; align=&quot;right&quot; src=&quot;/assets/img/fader.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Developing such a system is not a trivial task, as you might imagine. The system would have to continuously do the following, in real-time:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identify all sources of sound in a given environment&lt;/li&gt;
  &lt;li&gt;Identify the types of sounds that are coming from each source&lt;/li&gt;
  &lt;li&gt;Separate the sounds into their own channels&lt;/li&gt;
  &lt;li&gt;Apply appropriate equalization and dynamic range compression settings to make the sounds more ‘listenable’&lt;/li&gt;
  &lt;li&gt;Allow the user to adjust the gain of each source while preserving each sources’ spatial cues&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Source separation itself is a super difficult problem, but doing it in real-time for human listening (as opposed to machine listening) applications requires only a few milliseconds of delay between the microphone and the ear, making the problem orders of magnitude more difficult. Luckily, AL technologies are not constrained by having only two ears, the way humans are. We can use dozens or even hundreds of microphones (microphone arrays) to learn about a given auditory scene and process it accordingly.&lt;/p&gt;

&lt;h2 id=&quot;microphone-arrays&quot;&gt;Microphone Arrays&lt;/h2&gt;

&lt;p&gt;Though we may not think about it, we are constantly surrounded by microphones (see ‘A Word on Privacy’ section if this freaks you out as much as it did for me). Multiple microphones are built into our phones, laptops, smart speakers, hearing aids, smart headphones, audio conferencing equipment, and so much more. For example, the Amazon Echo has seven microphones and the Microsoft Kinect has four. These microphone arrays use subtle time-of-flight differences between signals at different sensors to determine what sounds are coming from what directions, track them as they move, and separate the signals spatially.&lt;/p&gt;

&lt;p&gt;In our proposed system, we used similar techniques to perform source separation by exploiting the redundancies of all the microphones in a given room. The source separation system, which would run on a powerful computer or the cloud, decides what sources are in the room, where they are, and how sound propagates from the sources to each user’s listening device. Then, each listening device uses this information to design its own beamformer that uses only its own nearby microphones. Beamformers are like flashlights but instead of emitting light in the form of a beam, we are collecting acoustic information for a given three-dimensional space.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;&lt;img height=&quot;200&quot; align=&quot;right&quot; src=&quot;/assets/img/room.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To test the proposed system, Ryan and I filled a medium-sized conference room with 160 lavelier microphones. There were four mannequins with 16 microphones mounted at various locations on their clothes (Ryan did some &lt;a href=&quot;https://publish.illinois.edu/augmentedlistening/acoustic-impulse-responses-for-wearable-audio-devices/&quot;&gt;other work&lt;/a&gt; on wearable microphone arrays) as well as 12 enclosures that simulated smart speakers distributed around the room that each had eight microphones. Ten loudspeakers were mounted roughly at head height to simulate a cocktail party scenario.&lt;/p&gt;

&lt;p&gt;We recorded both impulse responses (to learn about the room acoustics) and speech samples played at each of the ten speakers, as well as a mixture of the speakers to simulate a cocktail party scenario. The work resulted in a &lt;a href=&quot;https://arxiv.org/pdf/1912.05038.pdf&quot;&gt;CAMSAP paper&lt;/a&gt;, poster (below) and publicly-available &lt;a href=&quot;https://databank.illinois.edu/datasets/IDB-6216881#&quot;&gt;dataset&lt;/a&gt;. Check out some of the results on &lt;a href=&quot;http://ryanmcorey.com/demos/cooperative/cooperative_demo.html&quot;&gt;our demos page.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-word-on-privacy&quot;&gt;A Word on Privacy&lt;/h2&gt;

&lt;p&gt;Obviously, there are severe privacy concerns that have not been addressed in this work. For starters, a user would be able to listen to any sound source, even if it is on the other side of the room. Furthermore, a hacker would be able to hijack the contents of any conversation in a room for whatever purpose. Luckily, a great deal of research has been done into privacy-preserving inference algorithms (see Peter Kairouz’s &lt;a href=&quot;https://www.ideals.illinois.edu/bitstream/handle/2142/92686/KAIROUZ-DISSERTATION-2016.pdf?sequence=1&amp;amp;isAllowed=y&quot;&gt;Ph.D. thesis&lt;/a&gt;) where the algorithm could perform acoustic channel estimation without having direct access to the signal’s content. That is, there are ways of encrypting signals in a context such as an audio remixing system that would allow them to protect user privacy. Nonetheless, more research needs to be done into the appropriateness for such a system in a given context and its impact on user privacy before being implemented in a real-world setting.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img height=&quot;900&quot; src=&quot;/assets/img/camsap.png&quot; /&gt;
&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Cooperative Listening Devices with Distributed and Wearable Microphone Arrays</summary></entry><entry><title type="html">Integer Programming for Optimal Right Hand Guitar Fingerings</title><link href="http://localhost:4000/20181217/fingerings" rel="alternate" type="text/html" title="Integer Programming for Optimal Right Hand Guitar Fingerings" /><published>2018-12-17T00:00:00-05:00</published><updated>2018-12-17T00:00:00-05:00</updated><id>http://localhost:4000/20181217/fingerings</id><content type="html" xml:base="http://localhost:4000/20181217/fingerings">&lt;h2 id=&quot;integer-programming-for-optimal-right-hand-guitar-fingerings&quot;&gt;Integer Programming for Optimal Right Hand Guitar Fingerings&lt;/h2&gt;

&lt;p&gt;The Western canon of European classical music includes a substantial amount of scale-based, fingerstyle guitar music. When presented with a piece of sheet music, a classical guitarist must make a number of decisions regarding its performance. These decisions include notating the tablature, the left hand fingerings, and the right hand fingerings. This process, especially for right hand guitar fingerings, is often a trivial, yet cumbersome task.&lt;/p&gt;

&lt;p&gt;For example, consider the &lt;a href=&quot;https://youtu.be/dmc6KV0_UVM?t=273&quot;&gt;&lt;em&gt;III. Allegro Solemne&lt;/em&gt; movement&lt;/a&gt; from the 1921 piece “La Catedral” by Paraguayan composer Agustín Barrios. Due to its fast tempo, the performer must pay careful attention to the fingerings to adhere to proper technique while maintaining such a tempo. The problem then becomes how can we use math and computer science to determine optimal fingerings for this type of music?&lt;/p&gt;

&lt;p&gt;Bernd Tahon began to answer this question with this 2017 Master’s thesis &lt;a href=&quot;https://vibeserver.net/scripties/2017/Fingers%20to%20Frets%20-%20Master%20Thesis%20Bernd%20Tahon.pdf&quot;&gt;&lt;em&gt;Fingers to frets - A Mathematical Approach&lt;/em&gt;&lt;/a&gt; where he examined how to utilize math and computer science to optimize left hand finger assignments. In this project, we tackled the separate, yet related problem of right hand finger assignments.&lt;/p&gt;

&lt;p&gt;In fingerstyle music, the guitar is played using the thumb, index, middle, and ring fingers (not the pinky). At present, there is a notion within the classical guitar community of “proper right hand technique” which can be thought of as a set of rules that describe the physically optimal way of using the right hand to play scale-based, fingerstyle guitar music. These rules can be summarized as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Do not repeat fingers on consecutive notes&lt;/li&gt;
  &lt;li&gt;The thumb plays the bassline&lt;/li&gt;
  &lt;li&gt;Fingers stay in natural resting position&lt;/li&gt;
  &lt;li&gt;Avoid backwards crossings&lt;/li&gt;
  &lt;li&gt;Avoid ring-middle-ring alternation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We decided to formulate this problem as an integer program where each of these rules are a constraint imposed by the model. The integer program, then, consists of a minimization of a sum of penalty terms that are incurred by violating the “soft” constraints. The complete integer program can be found below:&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%5Ctext%7Bminimize%7D&quot; alt=&quot;\text{minimize}&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20p%5E%7BBC%7D_i%20%2B%20p%5ET_i%20%2B%20%5Cfrac%7B1%7D%7B2%7D%20p%5EF_i&quot; alt=&quot;\sum_{i=1}^{n} p^{BC}_i + p^T_i + \frac{1}{2} p^F_i&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%5Ctext%7Bsubject%20to%3A%7D&quot; alt=&quot;\text{subject to:}&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/b_i*f_i%3C4%2Bp%5ET_i%20%5Cqquad%20%5Cforall%20%5C%20i&quot; alt=&quot;b_i*f_i&amp;lt;4+p^T_i \qquad \forall \ i&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%7Cf_i-s_i%7C%20-%20(t_i*M)%20%5Cleq%20p_i%5EF%20%5Cqquad%20%5Cforall%20%5C%20i&quot; alt=&quot;|f_i-s_i| - (t_i*M) \leq p_i^F \qquad \forall \ i&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/-(s_i-s_%7Bi%2B1%7D)(f_i-f_%7Bi%2B1%7D)%5Cleq%20M*p%5E%7BBC%7D_i%20%5Cqquad%20%5Cforall%20%5C%20i%20%3C%20n-1&quot; alt=&quot;-(s_i-s_{i+1})(f_i-f_{i+1})\leq M*p^{BC}_i \qquad \forall \ i &amp;lt; n-1&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/f_i%20%2B%20f_%7Bi%2B1%7D%20%2B%20f_%7Bi%2B2%7D%20%5Cgeq%205%20%5Cqquad%20%5Cforall%20%5C%20i%20%3C%20n-2&quot; alt=&quot;f_i + f_{i+1} + f_{i+2} \geq 5 \qquad \forall \ i &amp;lt; n-2&quot; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&quot;https://tex.s2cms.ru/svg/%7Cf_i-f_%7Bi%2B1%7D%7C%20%5Cgeq%201%20%5Cqquad%20%5Cforall%20%5C%20i%20%3C%20n-1&quot; alt=&quot;|f_i-f_{i+1}| \geq 1 \qquad \forall \ i &amp;lt; n-1&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;An explanation of this model as well as other information about this project can be found in &lt;a href=&quot;/assets/docs/RHFingerings.pdf&quot;&gt;this paper.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We implemented the model with Gurobi Optimizer and tested it on some selected measures of “La Catedral”. Below are some examples of the results. The f &lt;sub&gt;i&lt;/sub&gt; correspond to the finger assignment of each note as returned by our model and the f &lt;sub&gt;i&lt;/sub&gt;* correspond to the optimal finger assignment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/measure1.png&quot; alt=&quot;drawing&quot; width=&quot;200&quot; style=&quot;float: center;&quot; /&gt; 
&lt;img src=&quot;/assets/img/measure2.png&quot; alt=&quot;drawing&quot; width=&quot;200&quot; style=&quot;float: center;&quot; /&gt; 
&lt;img src=&quot;/assets/img/measure3.png&quot; alt=&quot;drawing&quot; width=&quot;160&quot; style=&quot;float: center;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;https://github.com/mskarha/rhfingerings&quot;&gt;here&lt;/a&gt; to access the Github repository of the Python code used to generate the .lp file in order to use our model on your own music.&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Integer Programming for Optimal Right Hand Guitar Fingerings</summary></entry><entry><title type="html">Laser Interferometry for Loudspeaker Characterization</title><link href="http://localhost:4000/20171203/laser-interferometry" rel="alternate" type="text/html" title="Laser Interferometry for Loudspeaker Characterization" /><published>2017-12-03T00:00:00-05:00</published><updated>2017-12-03T00:00:00-05:00</updated><id>http://localhost:4000/20171203/laser-interferometry</id><content type="html" xml:base="http://localhost:4000/20171203/laser-interferometry">&lt;h2 id=&quot;laser-interferometry-for-loudspeaker-characterization&quot;&gt;Laser Interferometry for Loudspeaker Characterization&lt;/h2&gt;

&lt;p&gt;In this experiment, I set up Michelson interferometer to measure the frequency response of a commercial loudspeaker to high precision.&lt;/p&gt;

&lt;p&gt;In a Michelson interferometer, a light source (in our case, a He-Ne laser) is split into two arms with a beam splitter. Each of those beams is reflected back towards the beamsplitter via a mirror, which then combines their amplitudes using the superposition principle. The resulting interference pattern is then picked up by a photoelectric detector. Because of the small wavelength of our light, this technique can be used to measure extremely small displacements, in accordance with the path length difference between the two arms.&lt;/p&gt;

&lt;p&gt;I decided to mount one of the mirrors on the cone of my &lt;a href=&quot;http://www.jblpro.com/www/products/recording-broadcast/3-series/lsr305#.W4wu8NhKiRs&quot;&gt;JBL LSR305&lt;/a&gt; studio monitor that I use for mixing music. One of the main principles behind studio monitors is that the frequency response of the speakers must be as flat as possible so as to provide the most pure representation of whatever sound is being played. This is crucial for audio engineers who are dealing with very minute details in a recording. Typically, manufacturers of home stereo systems and other speakers go at great lengths to enhance audio quality by boosting certain bass, mid, and high frequencies. This renders normal speakers almost useless for audio production.&lt;/p&gt;

&lt;p&gt;Using this experimental setup, I was able to verify the flatness of the JBL LSR305 studio monitor by sweeping through a large range of frequencies and determining the cone displacement based on the interference pattern. &lt;a href=&quot;/assets/docs/Laser interferometric characterization for a vibrating speaker system good.pdf&quot;&gt;Here&lt;/a&gt; is a paper I wrote about the experimental process. Check out this image of the experimental setup:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/michelson.jpg&quot; alt=&quot;drawing&quot; width=&quot;450&quot; style=&quot;float: center;&quot; /&gt;&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;My proudest snapchat moment ever was when I set up a laser interferometer in physics lab to measure speaker cone displacement then played technotronic&amp;#39;s pump up the jam and took data with an oscilloscope built by &lt;a href=&quot;https://twitter.com/tektronix?ref_src=twsrc%5Etfw&quot;&gt;@tektronix&lt;/a&gt; &lt;a href=&quot;https://t.co/Tu06GeEF6M&quot;&gt;pic.twitter.com/Tu06GeEF6M&lt;/a&gt;&lt;/p&gt;&amp;mdash; skorched earth policy (@skarhaha56) &lt;a href=&quot;https://twitter.com/skarhaha56/status/1012424839993413632?ref_src=twsrc%5Etfw&quot;&gt;June 28, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Laser Interferometry for Loudspeaker Characterization</summary></entry><entry><title type="html">Chaotic Double Pendulum Synthesizer</title><link href="http://localhost:4000/20170831/chaotic-pendulum" rel="alternate" type="text/html" title="Chaotic Double Pendulum Synthesizer" /><published>2017-08-31T00:00:00-04:00</published><updated>2017-08-31T00:00:00-04:00</updated><id>http://localhost:4000/20170831/chaotic-pendulum</id><content type="html" xml:base="http://localhost:4000/20170831/chaotic-pendulum">&lt;h2 id=&quot;chaotic-double-pendulum-synthesizer&quot;&gt;Chaotic Double Pendulum Synthesizer&lt;/h2&gt;

&lt;p&gt;The Chaotic Double Pendulum Synthesizer is an attempt to find musicality in the laws of physics that govern our world.&lt;/p&gt;

&lt;p&gt;The double pendulum is a simple idea, but complex mathematically. The basic idea of a chaotic double pendulum is that it is a simple physical system (a pendulum attached to the end of another pendulum) that exhibits rich dynamic behavior (much more interesting motion that one would expect) with a strong sensitivity to initial conditions. The motion is often described by physicists as unexpectedly mesmerizing.&lt;/p&gt;

&lt;p&gt;The goal of this project is to capture that mesmerizing motion with electroacoustic music using Max/MSP motion tracking and sound processing. By feeding data about the position, velocity, and acceleration of the pendulums into a series of oscillators, we can “listen” to the chaos that governs some of the laws of physics of our universe.&lt;/p&gt;

&lt;p&gt;Click &lt;a href=&quot;http://scienceworld.wolfram.com/physics/DoublePendulum.html&quot;&gt;here&lt;/a&gt; to learn more about some of the math behind coupled pendula. Additionally, click &lt;a href=&quot;https://www.youtube.com/watch?v=AwT0k09w-jw&quot;&gt;here&lt;/a&gt; to watch a demo of a chaotic double pendulum.&lt;/p&gt;

&lt;p&gt;Click the image below to watch a brief demo of the synthesizer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=Op3IMovg4CM&quot;&gt;&lt;img src=&quot;http://img.youtube.com/vi/Op3IMovg4CM/0.jpg&quot; alt=&quot;synth&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mskarha/Chaotic-Double-Pendulum-Synthesizer&quot;&gt;Github Repository&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Chaotic Double Pendulum Synthesizer</summary></entry></feed>